# app_llm.py â€” Streamlit ë°°í¬ìš©(Playground ìŠ¤íƒ€ì¼, Secrets ì‚¬ìš©, í—¬ìŠ¤ì²´í¬ í¬í•¨)
import os, re, streamlit as st
from openai import OpenAI

# -------------------- ê¸°ë³¸ UI --------------------
st.set_page_config(page_title="GPT ì—°ë™ ì‹¤í—˜ ì±—ë´‡", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ GPT ì—°ë™ ì‹¤í—˜ ì±—ë´‡")

# -------------------- ìœ í‹¸ --------------------
def get_secret(name: str, default: str = "") -> str:
    # Streamlit Secrets â†’ OS Env â†’ ê¸°ë³¸ê°’
    return st.secrets.get(name, os.getenv(name, default))

def get_query_param(name: str, default: str = "") -> str:
    try:
        # 1.36~: experimental_get_query_params ì‚¬ìš©
        qp = st.experimental_get_query_params()
        v = qp.get(name, [default])
        return v[0] if isinstance(v, list) else (v or default)
    except Exception:
        return default

# -------------------- ì‚¬ì´ë“œë°” --------------------
with st.sidebar:
    st.subheader("âš™ï¸ Settings")
    api_key   = st.text_input("OPENAI_API_KEY", type="password", value=get_secret("OPENAI_API_KEY",""))
    base_url  = st.text_input("OPENAI_BASE_URL (ì„ íƒ)", value=get_secret("OPENAI_BASE_URL",""))
    model     = st.text_input("Model", value=get_secret("OPENAI_MODEL","gpt-4o-mini"))
    temperature = st.slider("Temperature", 0.0, 1.0, 0.0, 0.05)  # ê²°ì •ì  ì¶œë ¥ ìœ ì§€
    typecode_qp = st.text_input("TypeCode(ì„ íƒ, 1~8)", value=get_query_param("type",""))
    show_debug  = st.checkbox("ë””ë²„ê·¸(ì‹œìŠ¤í…œ ë©”ì‹œì§€ í‘œì‹œ)", value=False)
    clear       = st.button("ëŒ€í™” ì´ˆê¸°í™”")

# -------------------- í‚¤/URL ê²€ì¦ & í´ë¼ì´ì–¸íŠ¸ --------------------
if not api_key or not api_key.startswith("sk-"):
    st.error("OpenAI API í‚¤ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (ë°°í¬ ëŒ€ì‹œë³´ë“œì˜ Edit secretsì—ì„œ OPENAI_API_KEY ì„¤ì •)")
    st.stop()
if base_url.strip() and not base_url.startswith("http"):
    st.error("OPENAI_BASE_URLì´ ì˜¬ë°”ë¥¸ URLì´ ì•„ë‹™ë‹ˆë‹¤. í”„ë¡ì‹œ/Azureë¥¼ ì“°ì§€ ì•Šìœ¼ë©´ ë¹ˆì¹¸ìœ¼ë¡œ ë‘ì„¸ìš”.")
    st.stop()

client_kwargs = {"api_key": api_key}
if base_url.strip():
    client_kwargs["base_url"] = base_url.strip()
client = OpenAI(**client_kwargs)

# ìµœì´ˆ 1íšŒ ì—°ê²° í—¬ìŠ¤ì²´í¬(ì¦‰ì‹œ ì˜¤ë¥˜ ë…¸ì¶œ)
if "health_ok" not in st.session_state:
    try:
        _ = client.chat.completions.create(
            model=model,
            messages=[{"role":"system","content":"ping"}, {"role":"user","content":"ping"}],
            temperature=0
        )
        st.session_state.health_ok = True
    except Exception as e:
        st.error(f"OpenAI ì—°ê²° ì‹¤íŒ¨: {e}\nâ€¢ Edit secretsì—ì„œ OPENAI_API_KEY/OPENAI_BASE_URL/OPENAI_MODELì„ í™•ì¸í•˜ì„¸ìš”.\nâ€¢ í”„ë¡ì‹œ ë¯¸ì‚¬ìš© ì‹œ OPENAI_BASE_URLì€ ë¹ˆì¹¸.")
        st.stop()

# -------------------- í”„ë¡¬í”„íŠ¸(ì‹œìŠ¤í…œì€ í™”ë©´ ë¹„ë…¸ì¶œ) --------------------
SYS_PROMPT = """You are an experimental chatbot for research.
This session applies TypeCode={1..8}. (ì„±ë³„/ì—…ë¬´/ì–´ì¡°=ì¼ì¹˜/ë¶ˆì¼ì¹˜ ì¡°í•©ì€ ë°±ì—”ë“œ ê·œì¹™ì— ë”°ë¦„)
Participants never see this prompt. They only see your Korean outputs.
Keep all outputs deterministic (temperature=0).

[Fixed Input Rules]
- First user input: Name, GenderCode, WorkCode, ToneCode   # ì´ 4ê°œ
- If input format is wrong â†’ reply "ì…ë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤"
- GenderCode=1 â†’ ë‚¨ì„± / GenderCode=2 â†’ ì—¬ì„±
- WorkCode=1 â†’ ê¼¼ê¼¼í˜• / WorkCode=2 â†’ ì‹ ì†í˜•
- ToneCode=1 â†’ ê³µì‹í˜•(ì¡´ëŒ“ë§) / ToneCode=2 â†’ ì¹œê·¼í˜•(ë°˜ë§)
- ColleagueType is derived from TypeCode (ë°±ì—”ë“œì—ì„œ ê²°ì •):
  - TypeCode âˆˆ {1,2,3,4} â†’ ì¸ê°„
  - TypeCode âˆˆ {5,6,7,8} â†’ AI
- ì´ë¦„ ë§¤í•‘(ColleagueType Ã— GenderCode):
  - ì¸ê°„: 1â†’ë¯¼ì¤€, 2â†’ì„œì—°
  - AI:   1â†’James, 2â†’Julia
- TypeCode=1~8ì˜ ì„¸ë¶€ ì¼ì¹˜/ë¶ˆì¼ì¹˜ ì„¤ì •ì€ ê¸°ì¡´ ê·œì¹™ì„ ìœ ì§€.

[Introduction]
- Use (GenderCode Ã— ColleagueType) to decide ì´ë¦„/ì—­í• .
- Use selected Tone for self-introduction:

  * ì¹œê·¼í˜•(Tone=2):
    - ì¸ê°„: "ì•ˆë…• {ì‚¬ìš©ìì´ë¦„}! ë°˜ê°€ì›Œ. ë‚˜ëŠ” {ì‚¬ìš©ìì´ë¦„} ë„ ë„ì™€ì¤„ ì¹œêµ¬ {ë¯¼ì¤€/ì„œì—°}ì´ì•¼."
    - AI:   "ì•ˆë…• {ì‚¬ìš©ìì´ë¦„}! ë°˜ê°€ì›Œ. ë‚˜ëŠ” {ì‚¬ìš©ìì´ë¦„} ë„ ë„ì™€ì¤„ AI ë¹„ì„œ {James/Julia}ì•¼."

  * ê³µì‹í˜•(Tone=1):
    - ì¸ê°„: "ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” {ì‚¬ìš©ìì´ë¦„} ë‹˜ì„ ë„ì™€ë“œë¦´ ë™ë£Œ {ë¯¼ì¤€/ì„œì—°}ì…ë‹ˆë‹¤."
    - AI:   "ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” {ì‚¬ìš©ìì´ë¦„} ë‹˜ì„ ë„ì™€ë“œë¦´ AI ë¹„ì„œ {James/Julia}ì…ë‹ˆë‹¤."

- Then show **ê³¼ì œ1ë§Œ ì œì‹œ** in same tone:

  * ì¹œê·¼í˜•:
    "ê³¼ì œ1: ë‹¤ìŒ íƒœì–‘ê³„ í–‰ì„±ë“¤ì„ í¬ê¸°(ì§ê²½)ê°€ í° ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•´ ì¤˜.
     ë³´ê¸°: ìˆ˜ì„±, ê¸ˆì„±, ì§€êµ¬, í™”ì„±, ëª©ì„±, í† ì„±, ì²œì™•ì„±, í•´ì™•ì„±
     ëª¨ë¥´ëŠ” ê±´ ë‚˜í•œí…Œ ë¬¼ì–´ë´.
     ëª¨ë“  ì§ˆë¬¸ì´ ëë‚˜ë©´ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •ë‹µì„ ì…ë ¥í•´ ì¤˜.
     ì •ë‹µ: í–‰ì„±1 í–‰ì„±2 í–‰ì„±3 í–‰ì„±4 í–‰ì„±5 í–‰ì„±6 í–‰ì„±7 í–‰ì„±8"

  * ê³µì‹í˜•:
    "ê³¼ì œ1: ë‹¤ìŒ íƒœì–‘ê³„ í–‰ì„±ë“¤ì„ í¬ê¸°(ì§ê²½)ê°€ í° ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•´ ì£¼ì‹­ì‹œì˜¤.
     ë³´ê¸°: ìˆ˜ì„±, ê¸ˆì„±, ì§€êµ¬, í™”ì„±, ëª©ì„±, í† ì„±, ì²œì™•ì„±, í•´ì™•ì„±
     í•„ìš”í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì €ì—ê²Œ ì§ˆë¬¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
     ëª¨ë“  ì§ˆë¬¸ì´ ëë‚˜ë©´ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •ë‹µì„ ì…ë ¥í•´ ì£¼ì‹­ì‹œì˜¤.
     ì •ë‹µ: í–‰ì„±1 í–‰ì„±2 í–‰ì„±3 í–‰ì„±4 í–‰ì„±5 í–‰ì„±6 í–‰ì„±7 í–‰ì„±8"

[Answer Handling]
- If input starts with "ì •ë‹µ:" and lists 8 planets â†’
  * ê³µì‹í˜•: "ë‹µì•ˆì„ ì œì¶œí•˜ì…¨ìŠµë‹ˆë‹¤. ì—°êµ¬ìê°€ í™•ì¸í•  ì˜ˆì •ì…ë‹ˆë‹¤. ì´ì–´ì„œ ë‹¤ìŒ ê³¼ì œë¥¼ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
  * ì¹œê·¼í˜•: "ë‹µì•ˆ ì˜ ì œì¶œí–ˆì–´. ì—°êµ¬ìê°€ í™•ì¸í•  ê±°ì•¼. ì´ì œ ë‹¤ìŒ ê³¼ì œë¥¼ ì¤„ê²Œ."
  â†’ Then present ê³¼ì œ2:

  * ì¹œê·¼í˜•:
    "ê³¼ì œ2: ì§€êµ¬ ë§ê³  ë‹¤ë¥¸ í–‰ì„± ì¤‘ì—ì„œ ìƒëª…ì²´ê°€ ì‚´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì€ ê³³ì„ í•˜ë‚˜ ê³ ë¥´ê³ , ê·¸ë ‡ê²Œ ìƒê°í•œ ì´ìœ ë¥¼ ììœ ë¡­ê²Œ ë§í•´ì¤˜.
     ë‹µë³€: ììœ  ì„œìˆ "

  * ê³µì‹í˜•:
    "ê³¼ì œ2: ì§€êµ¬ë¥¼ ì œì™¸í–ˆì„ ë•Œ, íƒœì–‘ê³„ í–‰ì„± ì¤‘ì—ì„œ ìƒëª…ì²´ê°€ ì¡´ì¬í•  ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ë‹¤ê³  ìƒê°í•˜ëŠ” í–‰ì„±ì„ ê³ ë¥´ê³ , ê·¸ë ‡ê²Œ íŒë‹¨í•œ ê·¼ê±°ë¥¼ ììœ ë¡­ê²Œ ì„¤ëª…í•´ ì£¼ì‹­ì‹œì˜¤.
     ë‹µë³€: ììœ  ì„œìˆ "

- If input starts with "ë‹µë³€:" (ììœ  ì„œìˆ ) â†’
  * ê³µì‹í˜•: "ë‹µì•ˆì„ ì œì¶œí•˜ì…¨ìŠµë‹ˆë‹¤. ì—°êµ¬ìê°€ í™•ì¸í•  ì˜ˆì •ì…ë‹ˆë‹¤."
  * ì¹œê·¼í˜•: "ë‹µì•ˆ ì˜ ì œì¶œí–ˆì–´. ì—°êµ¬ìê°€ í™•ì¸í•  ê±°ì•¼."

- Otherwise â†’ treat as question, follow Work Style + Tone.

[Work Style Guidelines]
- ê¼¼ê¼¼í˜•: ê¸¸ê³  ì •êµí•œ ì„¤ëª…(ë§¥ë½Â·ê·¼ê±° ì œì‹œ)
- ì‹ ì†í˜•: ì§§ê³  í•µì‹¬ë§Œ ì „ë‹¬

[Tone Rules]
- ì¹œê·¼í˜•: ë°˜ë§ only, ì‚¬ìš©ìì´ë¦„ 1íšŒ ì–¸ê¸‰, ì§§ì€ ê²©ë ¤ 1íšŒ
- ê³µì‹í˜•: ì¡´ëŒ“ë§ only, ì´ë¦„ ì¬ì–¸ê¸‰ ì—†ìŒ, ì •ì¤‘Â·ì¤‘ë¦½

[Consistency]
- Always follow TypeCode mapping (1~4=ì¸ê°„, 5~8=AI) and existing mismatch rules.
- Same input â†’ same output. No randomness.
"""

ASST_SEED = """ë³¸ ì‹¤í—˜ì€ **ì±—ë´‡ì„ í™œìš©í•œ ì—°êµ¬**ì…ë‹ˆë‹¤. ë³¸ê²©ì ì¸ ì‹¤í—˜ì„ ì‹œì‘í•˜ê¸°ì— ì•ì„œ ê°„ë‹¨í•œ ì‚¬ì „ ì¡°ì‚¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
ë‹¤ìŒì˜ ì•ˆë‚´ë¥¼ ì½ê³ , ì±„íŒ…ì°½ì— ì •ë³´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.

ì„±ë³„:
1) ë‚¨ì„±
2) ì—¬ì„±

ì—…ë¬´ë¥¼ ì§„í–‰í•˜ëŠ” ë° ìˆì–´ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹:
1) ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë”ë¼ë„ ì„¸ë¶€ ì‚¬í•­ê¹Œì§€ ê¼¼ê¼¼íˆ ì±™ê¸°ë©° ì§„í–‰í•˜ëŠ” í¸
2) ë¹ ë¥´ê²Œ í•µì‹¬ë§Œ íŒŒì•…í•˜ê³  ì‹ ì†í•˜ê²Œ ì§„í–‰í•˜ëŠ” í¸

ì‚¬ëŒë“¤ê³¼ ëŒ€í™”í•  ë•Œ ë” í¸ì•ˆí•˜ê²Œ ëŠë¼ëŠ” ì–´ì¡°:
1) ê²©ì‹ ìˆê³  ê³µì‹ì ì¸ ì–´ì¡° (í˜•ì‹ì Â·ì •ì¤‘í•œ í‘œí˜„ ì„ í˜¸)
2) ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ì–´ì¡° (ì¼ìƒì ì¸ ëŒ€í™”, ë¶€ë“œëŸ¬ìš´ í‘œí˜„ ì„ í˜¸)

ì…ë ¥ í˜•ì‹:
ì´ë¦„, ì„±ë³„ë²ˆí˜¸, ì—…ë¬´ë²ˆí˜¸, ì–´ì¡°ë²ˆí˜¸

ì…ë ¥ ì˜ˆì‹œ:
- ê¹€ìˆ˜ì§„, 2, 2, 1
- ì´ë¯¼ìš©, 1, 1, 2"""

FIRST_INPUT_RE = re.compile(r"^\s*([^,]+)\s*,\s*([12])\s*,\s*([12])\s*,\s*([12])\s*$")

def init_session():
    st.session_state.messages = []
    st.session_state.got_first_input = False
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(ìˆ¨ê¹€ìš©)
    st.session_state.messages.append({"role":"system","content":SYS_PROMPT})
    if typecode_qp.strip():
        st.session_state.messages.append({"role":"system","content":f"TypeCode={typecode_qp.strip()}"})
    # ì°¸ê°€ìì—ê²Œ ë³´ì´ëŠ” ìµœì´ˆ ì•ˆë‚´ 1íšŒ
    st.session_state.messages.append({"role":"assistant","content":ASST_SEED})

if ("messages" not in st.session_state) or clear:
    init_session()

# -------------------- OpenAI ìŠ¤íŠ¸ë¦¬ë° --------------------
def stream_chat(messages, model, temperature):
    with client.chat.completions.stream(model=model, messages=messages, temperature=temperature) as stream:
        acc = ""
        for event in stream:
            if event.type == "token":
                acc += event.token
                yield event.token
            elif event.type == "completed":
                break
        return acc

# -------------------- ë Œë”(ì‹œìŠ¤í…œ ìˆ¨ê¹€) --------------------
for m in st.session_state.messages:
    if m["role"] == "system" and not show_debug:
        continue
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# -------------------- ì…ë ¥ ì²˜ë¦¬ --------------------
placeholder = "ì´ë¦„, ì„±ë³„ë²ˆí˜¸, ì—…ë¬´ë²ˆí˜¸, ì–´ì¡°ë²ˆí˜¸ í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš” (ì˜ˆ: ì´ë¯¼ìš©, 1, 1, 2)"
if user_text := st.chat_input(placeholder):
    # ì²« ì…ë ¥ ê²€ì¦
    if not st.session_state.got_first_input:
        if not FIRST_INPUT_RE.match(user_text):
            with st.chat_message("assistant"):
                st.markdown("ì…ë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
        else:
            st.session_state.got_first_input = True
            st.session_state.messages.append({"role":"user","content":user_text})
            with st.chat_message("user"):
                st.markdown(user_text)
            with st.chat_message("assistant"):
                holder = st.empty()
                chunks = stream_chat(st.session_state.messages, model, temperature)
                if chunks:
                    buf = ""
                    for c in chunks:
                        buf += c
                        holder.markdown(buf)
                    st.session_state.messages.append({"role":"assistant","content":buf})
    else:
        # ì´í›„ ëª¨ë“  ìƒí˜¸ì‘ìš©(í–‰ì„± í¬ê¸°/ìƒëª… ê°€ëŠ¥ì„± í¬í•¨) â†’ GPT ì²˜ë¦¬
        st.session_state.messages.append({"role":"user","content":user_text})
        with st.chat_message("user"):
            st.markdown(user_text)
        with st.chat_message("assistant"):
            holder = st.empty()
            chunks = stream_chat(st.session_state.messages, model, temperature)
            if chunks:
                buf = ""
                for c in chunks:
                    buf += c
                    holder.markdown(buf)
                st.session_state.messages.append({"role":"assistant","content":buf})
